{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a28ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736adea8",
   "metadata": {},
   "source": [
    "This note and dataset used is taken from these links:\n",
    "* https://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n",
    "* https://online.stat.psu.edu/stat505/lesson/10\n",
    "* https://www.data.gov.my/data/dataset/754d77ae-1dfb-4d56-a0aa-d37cffec4ff1/resource/fd1308bc-954f-4b89-a3f5-ede0ee4ec343/download/m-20210309040336_202211160300310_2008-2022-flows-of-foreign-direct-investment-in-malaysia_blocks.csv\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "\n",
    "Now, we are going to see one of the most applied technique in modern statistics ~ the Naive Bayes Classifier.\n",
    "\n",
    "This technique is a part of tools used in statistics classification problem. The main theory underlying behind it, which is the Bayes Theorem is used widely in other classifier such as the cluster analysis and discriminant analysis. The theorem is so simple that I feel like a skimp if not state it in this note. The Bayes Theorem is\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)\\times P(A)}{P(B)} $$\n",
    "\n",
    "where \n",
    "\n",
    "P(A|B) = Probability of A given B is occuring,\n",
    "\n",
    "P(A) = Probability of A is occuring.\n",
    "\n",
    "For classification problem, this theorem is easier to understand if we think of it in terms of priori and posterior distribution. If we want to know what is the probability of our observed data $x_i$ to fall into Category A given that we observed some feature $\\textbf{X}$ (this is A|B), we can calculate it if we know what is the probability of seeing feature $\\textbf{X}$ given it is category A (this is P(B|A)) multiply with probability of seeing category A. The left side of the equation is the posterior probability and the right side is the priori probability ~ the right side is calculated using information that we have in hand, in our case, it is the data collected previously for $x_i$.\n",
    "\n",
    "For each feature $\\textbf{X}$, we can then assume that it is independent among each other (meaning that if I just use 2 features to predict the category of A, the occurence of feature $X_1$ is not dependent on the occurence of feature $X_2$). This will further simplify our calculation above since for independency,\n",
    "\n",
    "$$ P(X_1 \\cap X_2) = P(X_1) \\times P(X_2)$$.\n",
    "\n",
    "And thus, our new formula for Bayes Theorem will looks like this,\n",
    "\n",
    "$$ P(A|\\textbf{X}) = \\frac{P(\\textbf{X}|A)\\times P(A)}{P(\\textbf{X})} $$\n",
    "\n",
    "$$ P(A|\\textbf{X}) = \\frac{P(X_1|A)P(X_2|A)\\ldots P(X_n|A)\\times P(A)}{P(\\textbf{X})} $$\n",
    "\n",
    "$$ P(A|\\textbf{X}) = \\frac{\\prod_{i=1}^{n} P(X_i|A)\\times P(A)}{P(\\textbf{X})} $$\n",
    "\n",
    "$$ P(A|\\textbf{X}) = \\frac{P(A) \\times \\prod_{i=1}^{n} P(X_i|A)}{P(\\textbf{X})} $$\n",
    "\n",
    "Please remember that in the example above, we just simply calculate the posterior probability of $x_i$ to fall into category A. If we have other category such as B, C and D, we'll need to find the posterior probability for each category and then choose the highest probability as our predicted category. (i.e, category with the highest posterior probability will be assigned to our $x_i$).\n",
    "\n",
    "There are many types of NB classifier but for this note, I will use the Gaussian Naive Bayes classifier where I will use the Gaussian Distribution (Normal distribution) to calculate each feature's probability. i.e, this part $P(X_i|A)$ is calculated using the normal distribution. \n",
    "\n",
    "We use gaussian NB if we have to make prediction based on features which are stored in numerical value.\n",
    "\n",
    "Next, we'll going through a simulation on how the NB works. I will use the data from DOSM which is the \"Flows of FDI in Malaysia by blocks of country\" and predict the type of country block given that we observed a specific amount of credit, debit and net of debit-credit of the country's FDI into Malaysia.\n",
    "\n",
    "As usual, before running any analysis, we need to make sure whether we need to do any testing to see if our data meet certain assumptions or not. However, for this note, I will not check the assumption because I just want to focus on the method/steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3414a216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>blocks</th>\n",
       "      <th>Category</th>\n",
       "      <th>Countries</th>\n",
       "      <th>credit</th>\n",
       "      <th>debit</th>\n",
       "      <th>net</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>Total East Asia</td>\n",
       "      <td>Total Country</td>\n",
       "      <td>17175</td>\n",
       "      <td>14129</td>\n",
       "      <td>3046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>of which</td>\n",
       "      <td>China, People's Republic of</td>\n",
       "      <td>1116</td>\n",
       "      <td>914</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>of which</td>\n",
       "      <td>Hong Kong, SAR</td>\n",
       "      <td>5602</td>\n",
       "      <td>4825</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>of which</td>\n",
       "      <td>Japan</td>\n",
       "      <td>7911</td>\n",
       "      <td>6129</td>\n",
       "      <td>1783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>of which</td>\n",
       "      <td>Korea, Republic of</td>\n",
       "      <td>886</td>\n",
       "      <td>880</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year     blocks         Category                    Countries  credit  \\\n",
       "0  2008  East Asia  Total East Asia                Total Country   17175   \n",
       "1  2008  East Asia         of which  China, People's Republic of    1116   \n",
       "2  2008  East Asia         of which               Hong Kong, SAR    5602   \n",
       "3  2008  East Asia         of which                        Japan    7911   \n",
       "4  2008  East Asia         of which           Korea, Republic of     886   \n",
       "\n",
       "   debit   net  \n",
       "0  14129  3046  \n",
       "1    914   201  \n",
       "2   4825   777  \n",
       "3   6129  1783  \n",
       "4    880     6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"https://www.data.gov.my/data/dataset/754d77ae-1dfb-4d56-a0aa-d37cffec4ff1/resource/fd1308bc-954f-4b89-a3f5-ede0ee4ec343/download/m-20210309040336_202211160300310_2008-2022-flows-of-foreign-direct-investment-in-malaysia_blocks.csv\"\n",
    "data = pd.read_csv(path)\n",
    "data = data.rename(columns = {'Blocks of countries':'blocks','Credit RM Million':'credit','Debit RM Million':'debit','Net RM Million':'net'})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1250d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping relevant data only. We don't want the Total for each block.\n",
    "data = data[data['Category']=='of which']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646fc0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into train & test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = data[['credit','debit','net']], data['blocks']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f50cd3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class gauss_NB:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, feature_train, result_train):\n",
    "        self.xtrain = feature_train\n",
    "        self.ytrain = result_train\n",
    "        self.category_prob = result_train.groupby(result_train).count() / len(result_train)\n",
    "        self.catname = np.array(self.category_prob.index.values)\n",
    "        self.featname = feature_train.columns.values.tolist()\n",
    "        \n",
    "    def get_param(self, feature, categ):\n",
    "        data = pd.merge(self.ytrain, self.xtrain, left_index=True, right_index=True)\n",
    "        self.params = data.groupby(data.iloc[:,0])[feature].describe()[['mean','std']]\n",
    "        return self.params.loc[categ]\n",
    "    \n",
    "    def gauss_prob(self,xi,ave,sd):\n",
    "        return np.exp(-0.5 * (((xi-ave)/sd)**2)) / (sd * np.sqrt(2*np.pi))\n",
    "       \n",
    "    def predict_cat(self, feature_test):\n",
    "        predict = []\n",
    "        for i in range(len(feature_test)):\n",
    "            post_prob = []\n",
    "            for cat in self.catname:\n",
    "                feat_prob = [self.gauss_prob(feature_test.iloc[i][[feat]], self.get_param(feat,cat)[0], self.get_param(feat,cat)[1]) for feat in self.featname]\n",
    "                feat_prob = np.prod(feat_prob)\n",
    "                cat_prob = feat_prob * self.category_prob.loc[cat]   \n",
    "                post_prob.append(cat_prob)\n",
    "            post_prob = np.asarray(post_prob)\n",
    "            highest_prob = np.argmax(post_prob)\n",
    "            label = self.catname[highest_prob]\n",
    "            predict.append(label)\n",
    "        return np.array(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a49532cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38596491228070173"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = gauss_NB()\n",
    "clf.fit(X_train, y_train)\n",
    "our_prediction = clf.predict_cat(X_test)\n",
    "accuracy = np.sum(y_test == our_prediction) / len(y_test)\n",
    "accuracy # very low accuracy but hey, we obtained a similar result with the sklearn package below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2e28899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38596491228070173"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = []\n",
    "for i in y_train:\n",
    "    if i == 'Southeast Asia':\n",
    "        y.append(1)\n",
    "    elif i == 'Europe':\n",
    "        y.append(2)\n",
    "    elif i == 'East Asia':\n",
    "        y.append(3)\n",
    "    elif i == 'North America':\n",
    "        y.append(4)\n",
    "    elif i == 'Latin America':\n",
    "        y.append(5)\n",
    "    elif i == 'Oceania':\n",
    "        y.append(6)\n",
    "\n",
    "ytest = []\n",
    "for i in y_test:\n",
    "    if i == 'Southeast Asia':\n",
    "        ytest.append(1)\n",
    "    elif i == 'Europe':\n",
    "        ytest.append(2)\n",
    "    elif i == 'East Asia':\n",
    "        ytest.append(3)\n",
    "    elif i == 'North America':\n",
    "        ytest.append(4)\n",
    "    elif i == 'Latin America':\n",
    "        ytest.append(5)\n",
    "    elif i == 'Oceania':\n",
    "        ytest.append(6)\n",
    "        \n",
    "xs = [[X_train.iloc[i,0], X_train.iloc[i,1], X_train.iloc[i,2]] for i in range(len(X_train))]\n",
    "xtest = [[X_test.iloc[i,0], X_test.iloc[i,1], X_test.iloc[i,2]] for i in range(len(X_test))]\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "result2 = model.fit(xs, y).predict(xtest)\n",
    "\n",
    "accuracy = np.sum(ytest == result2) / len(ytest)\n",
    "accuracy # this is the prediction from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1351cd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test == our_prediction) / np.sum(ytest == result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893733bb",
   "metadata": {},
   "source": [
    "And there you go. Our simulated naive bayes classifier works pretty well compare to the established package. Eventhough we use more time to run our prediction function, but it somehow obtain a similar result with package that is more optimized. What matter the most is that we all learn something new and now, we have a better understanding on how the Naive Bayes Classifier works.\n",
    "\n",
    "Below is the result that we obtained from the simulated NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30f48a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Southeast Asia', 'Europe', 'Latin America', 'Latin America',\n",
       "       'Latin America', 'Europe', 'North America', 'Oceania', 'Oceania',\n",
       "       'Europe', 'North America', 'Latin America', 'Latin America',\n",
       "       'Oceania', 'Latin America', 'Latin America', 'East Asia',\n",
       "       'Latin America', 'Latin America', 'Oceania', 'Latin America',\n",
       "       'Southeast Asia', 'Latin America', 'Latin America', 'Oceania',\n",
       "       'Latin America', 'Latin America', 'North America', 'Oceania',\n",
       "       'Latin America', 'East Asia', 'Latin America', 'Europe', 'Oceania',\n",
       "       'Latin America', 'Latin America', 'East Asia', 'Oceania',\n",
       "       'Latin America', 'Southeast Asia', 'East Asia', 'Latin America',\n",
       "       'Europe', 'Latin America', 'Latin America', 'East Asia', 'Oceania',\n",
       "       'Latin America', 'Oceania', 'Latin America', 'Latin America',\n",
       "       'Europe', 'Oceania', 'North America', 'East Asia', 'East Asia',\n",
       "       'Oceania'], dtype='<U14')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
